Unsupervised neural networks

    -> Feature Extraction

        - taking a dataset with info in an unfriendly form and translating it into a form this predictive-model friendly
            - ex. PCA

        - translating one set of features to a new set of different features
            - often smaller and more suitable for processing

        - Ex. reducting images to lines, edges, or certain kinds of curves

        - features extracted are often fed into another model for a predictive task
            - modeling pipeline


    -> Feature Engineering

        - manually driven process of adding new feautures to an existing data set


    -> Restricted Boltzmann machines

        - common form of neural network

        - look like the single layer perceptrons but without the specified output variable to train on

        - work in two layers
            1. inputs, or visible nodes
            2. hidden layer, new set of derived features

        -> these networks aim to extract patterns through finding reasonably common aggregations within the data


    -> Autoencoders

        - have an additional layer of output compared to RBMs

        - often defined by the number of features they output

        - can have the same number of output features as input features
            - in this case, it is being used to better learn those features

        - can also have fewer output features
            - compressing original data

        - more output features
            - expanding the data

        - frequently, autoencoders will have the same number of output features as input features
            - translates through a hidden layer with fewer features

        -> autoencoder is designed to regenerate features, not necessarily reduce dimensions
            - improving robustness
            - grouping redundancies

            - dataset with 5 features goes into autoencoder and leaves with 5 features in a better modeling state

        - often used early in the modeling pipeline for pre-training neural networks
            - the process of taking the hidden middle layer generated by the autoencoder and using it as input for another neural network layer


    -> Examples in unsupervised neural networks

        - RBM is particularly useful working with image data

        - RBM is the only unsupervised neural network in SKlearn
